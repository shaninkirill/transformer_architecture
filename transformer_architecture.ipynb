{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Привет мир'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzier = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzie_seq = tokenzier.tokenize(inputs.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет', 'мир']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenzie_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = np.random.rand(len(tokenzie_seq), embeddings_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35440833, 0.4533334 , 0.40693244, 0.0910958 ],\n",
       "       [0.60112991, 0.62932783, 0.05967745, 0.48236352]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(input_embeddings):\n",
    "    seq_len, embeddings_dim = input_embeddings.shape\n",
    "\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    \n",
    "    div_term = np.exp(np.arange(0, embeddings_dim, 2) * - (np.log(10000.0) / embeddings_dim))\n",
    "    \n",
    "    PE = np.zeros((seq_len, embeddings_dim))\n",
    "    PE[:, 0::2] = np.sin(position * div_term) \n",
    "    \n",
    "    PE[:, 1::2] = np.cos(position * div_term) \n",
    "    \n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings += positional_encoding(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35440833, 1.4533334 , 0.40693244, 1.0910958 ],\n",
       "       [1.4426009 , 1.16963014, 0.06967728, 1.48231352]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heads(heads_numbers, heads_n, heads_m):\n",
    "    WK = []\n",
    "    WQ = []\n",
    "    WV = []\n",
    "\n",
    "    for _ in range(heads_numbers):\n",
    "        WK_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "        WQ_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "        WV_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "\n",
    "        WK.append(WK_temp)\n",
    "        WQ.append(WQ_temp)\n",
    "        WV.append(WV_temp)\n",
    "\n",
    "    return np.array(WK), np.array(WQ), np.array(WV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK, WQ, WV = create_heads(2, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1, WK2 = WK[0], WK[1]\n",
    "WQ1, WQ2 = WQ[0], WQ[1]\n",
    "WV1, WV2 = WV[0], WV[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.21467417, 2.89883753, 1.85243657],\n",
       "       [2.68190832, 4.09454455, 2.9945917 ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1 = input_embeddings @ WK1\n",
    "K1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.86026584, 2.89883753],\n",
       "       [0.        , 1.23930742, 4.09454455]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = input_embeddings @ WV1\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.30576997, 1.4533334 , 1.49802825],\n",
       "       [4.16422184, 1.16963014, 1.5519908 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = input_embeddings @ WQ1\n",
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.30918305, 19.30249328],\n",
       "       [15.48792679, 20.60474268]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = Q1 @ K1.T\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.15459153,  9.65124664],\n",
       "       [ 7.7439634 , 10.30237134]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = scores1 / np.sqrt(embeddings_dim)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.076093  , 0.923907  ],\n",
       "       [0.07186366, 0.92813634]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = softmax(scores1)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.28655801, 4.00355962],\n",
       "       [0.        , 1.28393176, 4.00861667]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = scores1 @ V1\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(input_embeddings, WK, WQ, WV):\n",
    "    K = input_embeddings @ WK\n",
    "    Q = input_embeddings @ WQ\n",
    "    V = input_embeddings @ WV\n",
    "    \n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(embeddings_dim)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.28655801, 4.00355962],\n",
       "       [0.        , 1.28393176, 4.00861667]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = attention(input_embeddings, WK1, WQ1, WV1)\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.207324  , 0.        , 2.75214514],\n",
       "       [1.21508683, 0.        , 2.75843142]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention2 = attention(input_embeddings, WK2, WQ2, WV2)\n",
    "attention2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.28655801, 4.00355962, 1.207324  , 0.        ,\n",
       "        2.75214514],\n",
       "       [0.        , 1.28393176, 4.00861667, 1.21508683, 0.        ,\n",
       "        2.75843142]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = np.concatenate([attention1, attention2], axis=1)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "    attentions = []\n",
    "    for i in range(len(WK)):\n",
    "        WK_cur = WK[i]\n",
    "        WQ_cur = WQ[i]\n",
    "        WV_cur = WV[i]\n",
    "\n",
    "        scores_cur = attention(input_embeddings, WK_cur, WQ_cur, WV_cur)\n",
    "        attentions.append(scores_cur)\n",
    "\n",
    "    W = np.random.rand(W_n, W_m)\n",
    "\n",
    "    return np.concatenate(attentions, axis=1) @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = multi_head_attention(input_embeddings, 2, 4, 3, 6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.60579383, 6.39852941, 3.40582287, 5.06714706],\n",
       "       [6.61622183, 6.42713585, 3.44224757, 5.08628001]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(input_embeddings, epsilon=1e-6):\n",
    "    mean = input_embeddings.mean(axis=-1, keepdims=True)\n",
    "    std = input_embeddings.std(axis=-1, keepdims=True)\n",
    "    \n",
    "    return (input_embeddings - mean) / (std + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.01692645,  1.35054195, -0.90377116,  0.57015566],\n",
       "       [ 0.7000467 ,  0.22415465, -1.69348228,  0.76928093]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 17.56046869,  33.40746145, -11.15778284, -32.08958889],\n",
       "       [ 17.62437233,  33.53126755, -11.16325082, -32.1339551 ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = feed_forward(Z, W1, b1, W2, b2)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    Z = multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "    W1 = np.random.randn(4, 8)\n",
    "    W2 = np.random.randn(8, 4)\n",
    "    b1 = np.random.randn(8)\n",
    "    b2 = np.random.randn(4)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "    return layer_norm(output, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0798119 , -0.78269173, -0.26355583,  0.98491762],\n",
       "       [ 0.07125516, -0.78104855, -0.25979459,  0.98593556]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_embeddings, n=6):\n",
    "    for _ in range(n):\n",
    "        input_embeddings = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "\n",
    "    return input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.80829931,  1.43183273, -2.20259763, -0.85643053],\n",
       "       [ 1.80829931,  1.43183273, -2.20259763, -0.85643053]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = encoder(input_embeddings)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_embedding = np.random.rand(len(tokenzie_seq), embeddings_dim) # start of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13147021, 1.64945748, 0.3395816 , 1.49986676],\n",
       "       [1.09633239, 0.91859958, 0.59780829, 1.75305504]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_embedding += positional_encoding(sos_embedding)\n",
    "sos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_self_attention = multi_head_attention(sos_embedding, 2, 4, 3, 6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.68737952, 8.04150133, 4.97379977, 5.61006371],\n",
       "       [6.75647136, 8.10109689, 5.00731091, 5.6778185 ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2632541 ,  1.5611646 , -1.21955645, -0.07835404],\n",
       "       [ 0.30604852,  1.25665889, -1.52503004, -0.03767737]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_self_attention = layer_norm(decoder_self_attention + sos_embedding)\n",
    "decoder_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n",
    "    K = encoder_output @ WK    \n",
    "    V = encoder_output @ WV    \n",
    "    Q = attention_input @ WQ   \n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(embeddings_dim)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_encoder_decoder_attention(encoder_output, attention_input, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "    attentions = []\n",
    "    for i in range(len(WK)):\n",
    "        WK_cur = WK[i]\n",
    "        WQ_cur = WQ[i]\n",
    "        WV_cur = WV[i]\n",
    "        \n",
    "        scores_cur = encoder_decoder_attention(encoder_output, attention_input, WK_cur, WQ_cur, WV_cur)\n",
    "        attentions.append(scores_cur)\n",
    "    \n",
    "    W = np.random.rand(W_n, W_m)\n",
    "    concatenated_attention = np.concatenate(attentions, axis=1) @ W\n",
    "\n",
    "    return concatenated_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.02570883, -0.8047363 , -1.93070959, -3.09149961],\n",
       "       [-4.02570883, -0.8047363 , -1.93070959, -3.09149961]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_encoder_decoder = multi_head_encoder_decoder_attention(output_encoder, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(decoder_inputs, encoder_outputs, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    Z = multi_head_attention(decoder_inputs, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "    W1 = np.random.randn(4, 8)\n",
    "    W2 = np.random.randn(8, 4)\n",
    "    b1 = np.random.randn(8)\n",
    "    b2 = np.random.randn(4)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "    output = layer_norm(output, Z)\n",
    "\n",
    "    Z_encoder_decoder = multi_head_encoder_decoder_attention(encoder_outputs, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "\n",
    "    Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z)\n",
    "\n",
    "    output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z_encoder_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00811647, -0.09434004, -1.58766331,  0.67388687],\n",
       "       [ 1.00738228, -0.08646716, -1.59042641,  0.66951129]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_decoder = decoder_layer(sos_embedding, output_encoder, 2, 4, 3, 6, 4)\n",
    "output_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(decoder_input, output_encoder, n=6):\n",
    "    for _ in range(n):\n",
    "        decoder_input = decoder_layer(decoder_input, output_encoder, 2, 4, 3, 6, 4)\n",
    "\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.67847945,  0.35519744,  0.96247322,  0.3608088 ],\n",
       "       [-1.67847968,  0.35519772,  0.96247245,  0.36080951]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs = decoder(sos_embedding, output_decoder)\n",
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, W, b):\n",
    "    return np.dot(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probabilities(decoder_outputs, embeddings_dim, vocabulary_length):\n",
    "    for decoder_output in decoder_outputs:\n",
    "        linear_output = linear(decoder_output, np.random.randn(embeddings_dim, vocabulary_length), np.random.randn(vocabulary_length))\n",
    "\n",
    "        softmax_output = softmax(linear_output, axis=0)\n",
    "\n",
    "        print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11935165 0.21525809 0.66339475 0.00199552]\n",
      "[0.19325242 0.62772799 0.00160382 0.17741577]\n"
     ]
    }
   ],
   "source": [
    "output_probabilities(decoder_outputs, embeddings_dim, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
