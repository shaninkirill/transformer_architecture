{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Привет мир'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzier = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzie_seq = tokenzier.tokenize(inputs.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет', 'мир']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenzie_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = np.random.rand(len(tokenzie_seq), embeddings_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61591788, 0.87259507, 0.02572226, 0.77880353],\n",
       "       [0.02423783, 0.44503231, 0.84729367, 0.3490508 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(input_embeddings):\n",
    "    seq_len, embeddings_dim = input_embeddings.shape\n",
    "\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    \n",
    "    div_term = np.exp(np.arange(0, embeddings_dim, 2) * - (np.log(10000.0) / embeddings_dim))\n",
    "    \n",
    "    PE = np.zeros((seq_len, embeddings_dim))\n",
    "    PE[:, 0::2] = np.sin(position * div_term) \n",
    "    \n",
    "    PE[:, 1::2] = np.cos(position * div_term) \n",
    "    \n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings += positional_encoding(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61591788, 1.87259507, 0.02572226, 1.77880353],\n",
       "       [0.86570881, 0.98533462, 0.8572935 , 1.3490008 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heads(heads_numbers, heads_n, heads_m):\n",
    "    WK = []\n",
    "    WQ = []\n",
    "    WV = []\n",
    "\n",
    "    for _ in range(heads_numbers):\n",
    "        WK_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "        WQ_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "        WV_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "\n",
    "        WK.append(WK_temp)\n",
    "        WQ.append(WQ_temp)\n",
    "        WV.append(WV_temp)\n",
    "\n",
    "    return np.array(WK), np.array(WQ), np.array(WV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK, WQ, WV = create_heads(2, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1, WK2 = WK[0], WK[1]\n",
    "WQ1, WQ2 = WQ[0], WQ[1]\n",
    "WV1, WV2 = WV[0], WV[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61591788, 1.89831733, 2.39472141],\n",
       "       [0.86570881, 1.84262812, 2.21470961]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1 = input_embeddings @ WK1\n",
    "K1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.87259507, 0.02572226, 2.42044367],\n",
       "       [0.98533462, 0.8572935 , 3.07200312]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = input_embeddings @ WV1\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.6513986 , 4.29303874, 4.29303874],\n",
       "       [2.33433542, 4.05733774, 4.05733774]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = input_embeddings @ WQ1\n",
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.6791433 , 20.57935604],\n",
       "       [18.85606702, 18.48284428]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = Q1 @ K1.T\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.33957165, 10.28967802],\n",
       "       [ 9.42803351,  9.24142214]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = scores1 / np.sqrt(embeddings_dim)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51247082, 0.48752918],\n",
       "       [0.54651793, 0.45348207]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = softmax(scores1)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.44002971, 0.43113751, 2.73809791],\n",
       "       [1.47023836, 0.40282491, 2.7159142 ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = scores1 @ V1\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(input_embeddings, WK, WQ, WV):\n",
    "    K = input_embeddings @ WK\n",
    "    Q = input_embeddings @ WQ\n",
    "    V = input_embeddings @ WV\n",
    "    \n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(embeddings_dim)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.44002971, 0.43113751, 2.73809791],\n",
       "       [1.47023836, 0.40282491, 2.7159142 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = attention(input_embeddings, WK1, WQ1, WV1)\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.47146546, 4.11216441, 1.19172126],\n",
       "       [1.29878722, 4.14980252, 1.3334042 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention2 = attention(input_embeddings, WK2, WQ2, WV2)\n",
    "attention2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.44002971, 0.43113751, 2.73809791, 1.47146546, 4.11216441,\n",
       "        1.19172126],\n",
       "       [1.47023836, 0.40282491, 2.7159142 , 1.29878722, 4.14980252,\n",
       "        1.3334042 ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = np.concatenate([attention1, attention2], axis=1)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "    attentions = []\n",
    "    for i in range(len(WK)):\n",
    "        WK_cur = WK[i]\n",
    "        WQ_cur = WQ[i]\n",
    "        WV_cur = WV[i]\n",
    "\n",
    "        scores_cur = attention(input_embeddings, WK_cur, WQ_cur, WV_cur)\n",
    "        attentions.append(scores_cur)\n",
    "\n",
    "    W = np.random.rand(W_n, W_m)\n",
    "\n",
    "    return np.concatenate(attentions, axis=1) @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = multi_head_attention(input_embeddings, 2, 4, 3, 6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.26425771, 5.28549175, 3.01338395, 3.40271705],\n",
       "       [5.32776873, 5.3597962 , 3.01425016, 3.43668358]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(input_embeddings, epsilon=1e-6):\n",
    "    mean = input_embeddings.mean(axis=-1, keepdims=True)\n",
    "    std = input_embeddings.std(axis=-1, keepdims=True)\n",
    "    \n",
    "    return (input_embeddings - mean) / (std + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.58517879,  1.02276702, -1.34034693,  0.9027587 ],\n",
       "       [-0.74406903, -0.14518267, -0.78619886,  1.67545056]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-17.72807794, -18.82421232,  -6.29019925,  16.93018031],\n",
       "       [-17.95186365, -19.00824948,  -6.25756062,  17.11355849]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = feed_forward(Z, W1, b1, W2, b2)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    Z = multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "    W1 = np.random.randn(4, 8)\n",
    "    W2 = np.random.randn(8, 4)\n",
    "    b1 = np.random.randn(8)\n",
    "    b2 = np.random.randn(4)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "    return layer_norm(output, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.46178134,  0.08327153,  1.20373282, -0.51715907],\n",
       "       [-0.46077232,  0.08554531,  1.20272531, -0.51921085]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_embeddings, n=6):\n",
    "    for _ in range(n):\n",
    "        input_embeddings = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "\n",
    "    return input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.83155033,  2.17455561, -3.01334648,  0.10501041],\n",
       "       [-0.83155033,  2.17455561, -3.01334648,  0.10501041]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = encoder(input_embeddings)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_embedding = np.random.rand(len(tokenzie_seq), embeddings_dim) # start of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82912078, 1.92927165, 0.54918888, 1.92408717],\n",
       "       [1.61825237, 1.09771491, 0.52366311, 1.66924314]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_embedding += positional_encoding(sos_embedding)\n",
    "sos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_self_attention = multi_head_attention(sos_embedding, 2, 4, 3, 6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.66785665, 6.59645669, 7.7406441 , 6.94982241],\n",
       "       [9.62822046, 6.55469488, 7.70815099, 6.91905955]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.68107402, -0.60374038, -0.87715946, -0.20017418],\n",
       "       [ 1.68044953, -0.92652662, -0.50625207, -0.24767084]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_self_attention = layer_norm(decoder_self_attention + sos_embedding)\n",
    "decoder_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n",
    "    K = encoder_output @ WK    \n",
    "    V = encoder_output @ WV    \n",
    "    Q = attention_input @ WQ   \n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(embeddings_dim)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_encoder_decoder_attention(encoder_output, attention_input, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "    attentions = []\n",
    "    for i in range(len(WK)):\n",
    "        WK_cur = WK[i]\n",
    "        WQ_cur = WQ[i]\n",
    "        WV_cur = WV[i]\n",
    "        \n",
    "        scores_cur = encoder_decoder_attention(encoder_output, attention_input, WK_cur, WQ_cur, WV_cur)\n",
    "        attentions.append(scores_cur)\n",
    "    \n",
    "    W = np.random.rand(W_n, W_m)\n",
    "    concatenated_attention = np.concatenate(attentions, axis=1) @ W\n",
    "\n",
    "    return concatenated_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.7473543 , -0.74219627,  0.01939987, -1.82984868],\n",
       "       [-1.7473543 , -0.74219627,  0.01939987, -1.82984868]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_encoder_decoder = multi_head_encoder_decoder_attention(output_encoder, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(decoder_inputs, encoder_outputs, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    Z = multi_head_attention(decoder_inputs, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "    W1 = np.random.randn(4, 8)\n",
    "    W2 = np.random.randn(8, 4)\n",
    "    b1 = np.random.randn(8)\n",
    "    b2 = np.random.randn(4)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "    output = layer_norm(output, Z)\n",
    "\n",
    "    Z_encoder_decoder = multi_head_encoder_decoder_attention(encoder_outputs, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "\n",
    "    Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z)\n",
    "\n",
    "    output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z_encoder_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.19131474,  1.52620143,  0.14510851, -0.4799952 ],\n",
       "       [-1.19027205,  1.52875507,  0.13804366, -0.47652668]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_decoder = decoder_layer(sos_embedding, output_encoder, 2, 4, 3, 6, 4)\n",
    "output_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(decoder_input, output_encoder, n=6):\n",
    "    for _ in range(n):\n",
    "        decoder_input = decoder_layer(decoder_input, output_encoder, 2, 4, 3, 6, 4)\n",
    "\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07797279, -1.45344327,  1.36102441,  0.17039166],\n",
       "       [-0.07797206, -1.45344378,  1.36102388,  0.17039196]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs = decoder(sos_embedding, output_decoder)\n",
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, W, b):\n",
    "    return np.dot(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probabilities(decoder_outputs, embeddings_dim, vocabulary_length):\n",
    "    for decoder_output in decoder_outputs:\n",
    "        linear_output = linear(decoder_output, np.random.randn(embeddings_dim, vocabulary_length), np.random.randn(vocabulary_length))\n",
    "\n",
    "        softmax_output = softmax(linear_output, axis=0)\n",
    "\n",
    "        print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04761597 0.34187069 0.37068454 0.23982879]\n",
      "[0.87256586 0.0350587  0.02668308 0.06569236]\n"
     ]
    }
   ],
   "source": [
    "output_probabilities(decoder_outputs, embeddings_dim, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
