{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Привет мир'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzier = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzie_seq = tokenzier.tokenize(inputs.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет', 'мир']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenzie_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = np.random.rand(len(tokenzie_seq), embeddings_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11126297, 0.03067202, 0.64445921, 0.02823956],\n",
       "       [0.41482301, 0.87789299, 0.48406892, 0.39927242]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(input_embeddings):\n",
    "    seq_len, embeddings_dim = input_embeddings.shape\n",
    "\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    \n",
    "    div_term = np.exp(np.arange(0, embeddings_dim, 2) * - (np.log(10000.0) / embeddings_dim))\n",
    "    \n",
    "    PE = np.zeros((seq_len, embeddings_dim))\n",
    "    PE[:, 0::2] = np.sin(position * div_term) \n",
    "    \n",
    "    PE[:, 1::2] = np.cos(position * div_term) \n",
    "    \n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings += positional_encoding(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11126297, 1.03067202, 0.64445921, 1.02823956],\n",
       "       [1.256294  , 1.41819529, 0.49406875, 1.39922242]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heads(heads_numbers, heads_n, heads_m):\n",
    "    WK = []\n",
    "    WQ = []\n",
    "    WV = []\n",
    "\n",
    "    for _ in range(heads_numbers):\n",
    "        WK_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "        WQ_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "        WV_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "\n",
    "        WK.append(WK_temp)\n",
    "        WQ.append(WQ_temp)\n",
    "        WV.append(WV_temp)\n",
    "\n",
    "    return np.array(WK), np.array(WQ), np.array(WV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK, WQ, WV = create_heads(2, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1, WK2 = WK[0], WK[1]\n",
    "WQ1, WQ2 = WQ[0], WQ[1]\n",
    "WV1, WV2 = WV[0], WV[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75572218, 0.11126297, 0.        ],\n",
       "       [1.75036275, 1.256294  , 0.        ]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1 = input_embeddings @ WK1\n",
    "K1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11126297, 2.05891158, 1.78639421],\n",
       "       [1.256294  , 2.81741771, 3.16855804]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = input_embeddings @ WV1\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.78396175, 2.81463377, 1.67513123],\n",
       "       [3.14958516, 4.56778046, 1.91226404]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = input_embeddings @ WQ1\n",
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.66134399,  6.6585877 ],\n",
       "       [ 2.88843622, 11.25139172]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = Q1 @ K1.T\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.830672  , 3.32929385],\n",
       "       [1.44421811, 5.62569586]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = scores1 / np.sqrt(embeddings_dim)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07595485, 0.92404515],\n",
       "       [0.01504607, 0.98495393]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = softmax(scores1)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.16932334, 2.75980549, 3.06357599],\n",
       "       [1.23906578, 2.80600517, 3.1477619 ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = scores1 @ V1\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(input_embeddings, WK, WQ, WV):\n",
    "    K = input_embeddings @ WK\n",
    "    Q = input_embeddings @ WQ\n",
    "    V = input_embeddings @ WV\n",
    "    \n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(embeddings_dim)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.16932334, 2.75980549, 3.06357599],\n",
       "       [1.23906578, 2.80600517, 3.1477619 ]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = attention(input_embeddings, WK1, WQ1, WV1)\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.75841497, 1.37036437, 1.37036437],\n",
       "       [2.81136864, 1.39626383, 1.39626383]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention2 = attention(input_embeddings, WK2, WQ2, WV2)\n",
    "attention2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.16932334, 2.75980549, 3.06357599, 2.75841497, 1.37036437,\n",
       "        1.37036437],\n",
       "       [1.23906578, 2.80600517, 3.1477619 , 2.81136864, 1.39626383,\n",
       "        1.39626383]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = np.concatenate([attention1, attention2], axis=1)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "    attentions = []\n",
    "    for i in range(len(WK)):\n",
    "        WK_cur = WK[i]\n",
    "        WQ_cur = WQ[i]\n",
    "        WV_cur = WV[i]\n",
    "\n",
    "        scores_cur = attention(input_embeddings, WK_cur, WQ_cur, WV_cur)\n",
    "        attentions.append(scores_cur)\n",
    "\n",
    "    W = np.random.rand(W_n, W_m)\n",
    "\n",
    "    return np.concatenate(attentions, axis=1) @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = multi_head_attention(input_embeddings, 2, 4, 3, 6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.07265691, 5.48709887, 6.35985769, 6.60340461],\n",
       "       [6.17772097, 5.59596642, 6.47265251, 6.7092671 ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(input_embeddings, epsilon=1e-6):\n",
    "    mean = input_embeddings.mean(axis=-1, keepdims=True)\n",
    "    std = input_embeddings.std(axis=-1, keepdims=True)\n",
    "    \n",
    "    return (input_embeddings - mean) / (std + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.57381377,  0.86877517, -0.15727427,  0.86231287],\n",
       "       [ 0.30151163,  0.72840801, -1.70830055,  0.67838091]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.78340553, -18.86771623, -43.27758122,   7.57145825],\n",
       "       [  6.87007077, -19.21147192, -44.121359  ,   7.83127799]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = feed_forward(Z, W1, b1, W2, b2)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    Z = multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "    W1 = np.random.randn(4, 8)\n",
    "    W2 = np.random.randn(8, 4)\n",
    "    b1 = np.random.randn(8)\n",
    "    b2 = np.random.randn(4)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "    return layer_norm(output, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82659112, -0.44783026,  0.23297816,  0.99540385],\n",
       "       [-0.82727985, -0.44868242,  0.23212757,  0.99956175]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_embeddings, n=6):\n",
    "    for _ in range(n):\n",
    "        input_embeddings = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "\n",
    "    return input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21160828,  1.34875022, -1.51187604, -3.69495817],\n",
       "       [-0.21160828,  1.34875022, -1.51187604, -3.69495817]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = encoder(input_embeddings)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_embedding = np.random.rand(len(tokenzie_seq), embeddings_dim) # start of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60728785, 1.70612029, 0.32796654, 1.41379413],\n",
       "       [1.72342342, 1.0525022 , 0.03861733, 1.41376623]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_embedding += positional_encoding(sos_embedding)\n",
    "sos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_self_attention = multi_head_attention(sos_embedding, 2, 4, 3, 6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.01838214, 5.56991948, 9.1156004 , 5.14239635],\n",
       "       [4.97027017, 5.49251789, 8.99698968, 5.03849665]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.13615104,  0.03598941,  1.57543004, -0.47526842],\n",
       "       [-0.45439745, -0.59284717,  1.72647041, -0.67922579]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_self_attention = layer_norm(decoder_self_attention + sos_embedding)\n",
    "decoder_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n",
    "    K = encoder_output @ WK    \n",
    "    V = encoder_output @ WV    \n",
    "    Q = attention_input @ WQ   \n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(embeddings_dim)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_encoder_decoder_attention(encoder_output, attention_input, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "    attentions = []\n",
    "    for i in range(len(WK)):\n",
    "        WK_cur = WK[i]\n",
    "        WQ_cur = WQ[i]\n",
    "        WV_cur = WV[i]\n",
    "        \n",
    "        scores_cur = encoder_decoder_attention(encoder_output, attention_input, WK_cur, WQ_cur, WV_cur)\n",
    "        attentions.append(scores_cur)\n",
    "    \n",
    "    W = np.random.rand(W_n, W_m)\n",
    "    concatenated_attention = np.concatenate(attentions, axis=1) @ W\n",
    "\n",
    "    return concatenated_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-13.18388117, -11.24248645, -12.14853935, -11.9222812 ],\n",
       "       [-13.18388117, -11.24248645, -12.14853935, -11.9222812 ]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_encoder_decoder = multi_head_encoder_decoder_attention(output_encoder, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(decoder_inputs, encoder_outputs, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    Z = multi_head_attention(decoder_inputs, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "    W1 = np.random.randn(4, 8)\n",
    "    W2 = np.random.randn(8, 4)\n",
    "    b1 = np.random.randn(8)\n",
    "    b2 = np.random.randn(4)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "    output = layer_norm(output, Z)\n",
    "\n",
    "    Z_encoder_decoder = multi_head_encoder_decoder_attention(encoder_outputs, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "\n",
    "    Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z)\n",
    "\n",
    "    output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z_encoder_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06577883, -1.5820185 ,  1.17248329,  0.34375637],\n",
       "       [ 0.0662409 , -1.5817665 ,  1.17322211,  0.34230349]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_decoder = decoder_layer(sos_embedding, output_encoder, 2, 4, 3, 6, 4)\n",
    "output_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(decoder_input, output_encoder, n=6):\n",
    "    for _ in range(n):\n",
    "        decoder_input = decoder_layer(decoder_input, output_encoder, 2, 4, 3, 6, 4)\n",
    "\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69800707,  1.22136603, -1.2588952 , -0.6604779 ],\n",
       "       [ 0.69800706,  1.22136603, -1.25889523, -0.66047786]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs = decoder(sos_embedding, output_decoder)\n",
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, W, b):\n",
    "    return np.dot(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probabilities(decoder_outputs, embeddings_dim, vocabulary_length):\n",
    "    for decoder_output in decoder_outputs:\n",
    "        linear_output = linear(decoder_output, np.random.randn(embeddings_dim, vocabulary_length), np.random.randn(vocabulary_length))\n",
    "\n",
    "        softmax_output = softmax(linear_output, axis=0)\n",
    "\n",
    "        print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08062325 0.31888781 0.59731943 0.00316952]\n",
      "[0.00243138 0.97276776 0.00314775 0.02165311]\n"
     ]
    }
   ],
   "source": [
    "output_probabilities(decoder_outputs, embeddings_dim, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Embeddings():\n",
    "    def __init__(self, embeddings_dim):\n",
    "        self.embeddings_dim = embeddings_dim\n",
    "\n",
    "        self.words_list_ru = ['<sos>', 'привет', 'мир', '<eos>', '<unk>']\n",
    "        self.words_list_en = ['<sos>', 'hello', 'world', '<eos>', '<unk>']\n",
    "\n",
    "        self.create_input_ids_dict()\n",
    "        self.create_embeddings()\n",
    "\n",
    "\n",
    "    def create_input_ids_dict(self):\n",
    "        self.input_ids_dict = {}\n",
    "\n",
    "        for i in range(len(self.words_list_ru)):\n",
    "            self.input_ids_dict[i] = self.words_list_ru[i]\n",
    "\n",
    "\n",
    "    def get_input_id(self, token):\n",
    "        return list(self.input_ids_dict.keys())[list(self.input_ids_dict.values()).index(token)]\n",
    "    \n",
    "    \n",
    "    def get_input_ids(self, tokens):\n",
    "        input_ids = []\n",
    "        for token in tokens:\n",
    "            input_ids.append(self.get_input_id(token))\n",
    "\n",
    "        return input_ids\n",
    "    \n",
    "\n",
    "    def create_embeddings(self):\n",
    "        self.embeddings_dict = {}\n",
    "\n",
    "        for i in range(len(self.words_list_ru)):\n",
    "            self.embeddings_dict[i] = np.random.rand(self.embeddings_dim)\n",
    "\n",
    "\n",
    "    def get_embeddings(self, input_ids):\n",
    "        embeddings = []\n",
    "        for input_id in input_ids:\n",
    "            embeddings.append(self.embeddings_dict[input_id])\n",
    "\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def __call__(self, tokens):\n",
    "        self.input_ids = self.get_input_ids(tokens)\n",
    "\n",
    "        return self.get_embeddings(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer():\n",
    "    def __init__(self, \n",
    "                 embeddings_dim = 4,\n",
    "                 \n",
    "                 ):\n",
    "        self.tokenzier = WordPunctTokenizer()\n",
    "\n",
    "        self.embeddings_dim = embeddings_dim\n",
    "        self.embeddings = Embeddings(self.embeddings_dim)\n",
    "\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        tokenzie_seq = self.tokenzier.tokenize(inputs.lower())\n",
    "        self.input_embeddings = self.embeddings(tokenzie_seq)\n",
    "\n",
    "    \n",
    "    def positional_encoding(input_embeddings):\n",
    "        seq_len, embeddings_dim = input_embeddings.shape\n",
    "\n",
    "        position = np.arange(seq_len)[:, np.newaxis]\n",
    "        \n",
    "        div_term = np.exp(np.arange(0, embeddings_dim, 2) * - (np.log(10000.0) / embeddings_dim))\n",
    "        \n",
    "        PE = np.zeros((seq_len, embeddings_dim))\n",
    "        PE[:, 0::2] = np.sin(position * div_term) \n",
    "        \n",
    "        PE[:, 1::2] = np.cos(position * div_term) \n",
    "        \n",
    "        return PE\n",
    "    \n",
    "    \n",
    "    def create_heads(heads_numbers, heads_n, heads_m):\n",
    "        WK = []\n",
    "        WQ = []\n",
    "        WV = []\n",
    "\n",
    "        for _ in range(heads_numbers):\n",
    "            WK_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "            WQ_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "            WV_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "\n",
    "            WK.append(WK_temp)\n",
    "            WQ.append(WQ_temp)\n",
    "            WV.append(WV_temp)\n",
    "\n",
    "        return np.array(WK), np.array(WQ), np.array(WV)\n",
    "    \n",
    "\n",
    "    def softmax(x, axis=1):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "    def attention(input_embeddings, WK, WQ, WV):\n",
    "        K = input_embeddings @ WK\n",
    "        Q = input_embeddings @ WQ\n",
    "        V = input_embeddings @ WV\n",
    "        \n",
    "        scores = Q @ K.T\n",
    "        scores = scores / np.sqrt(embeddings_dim)\n",
    "        scores = softmax(scores)\n",
    "        scores = scores @ V\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "        WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "        attentions = []\n",
    "        for i in range(len(WK)):\n",
    "            WK_cur = WK[i]\n",
    "            WQ_cur = WQ[i]\n",
    "            WV_cur = WV[i]\n",
    "\n",
    "            scores_cur = attention(input_embeddings, WK_cur, WQ_cur, WV_cur)\n",
    "            attentions.append(scores_cur)\n",
    "\n",
    "        W = np.random.rand(W_n, W_m)\n",
    "\n",
    "        return np.concatenate(attentions, axis=1) @ W\n",
    "    \n",
    "\n",
    "    def layer_norm(input_embeddings, epsilon=1e-6):\n",
    "        mean = input_embeddings.mean(axis=-1, keepdims=True)\n",
    "        std = input_embeddings.std(axis=-1, keepdims=True)\n",
    "        \n",
    "        return (input_embeddings - mean) / (std + epsilon)\n",
    "\n",
    "\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "\n",
    "    def encoder_layer(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "        Z = multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "        W1 = np.random.randn(4, 8)\n",
    "        W2 = np.random.randn(8, 4)\n",
    "        b1 = np.random.randn(8)\n",
    "        b2 = np.random.randn(4)\n",
    "\n",
    "        output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "        return layer_norm(output, Z)\n",
    "    \n",
    "\n",
    "    def encoder(input_embeddings, n=6):\n",
    "        for _ in range(n):\n",
    "            input_embeddings = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "\n",
    "        return input_embeddings\n",
    "    \n",
    "\n",
    "    def encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n",
    "        K = encoder_output @ WK    \n",
    "        V = encoder_output @ WV    \n",
    "        Q = attention_input @ WQ   \n",
    "\n",
    "        scores = Q @ K.T\n",
    "        scores = scores / np.sqrt(embeddings_dim)\n",
    "        scores = softmax(scores)\n",
    "        scores = scores @ V\n",
    "        return scores\n",
    "    \n",
    "\n",
    "    def multi_head_encoder_decoder_attention(encoder_output, attention_input, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "        WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "        attentions = []\n",
    "        for i in range(len(WK)):\n",
    "            WK_cur = WK[i]\n",
    "            WQ_cur = WQ[i]\n",
    "            WV_cur = WV[i]\n",
    "            \n",
    "            scores_cur = encoder_decoder_attention(encoder_output, attention_input, WK_cur, WQ_cur, WV_cur)\n",
    "            attentions.append(scores_cur)\n",
    "        \n",
    "        W = np.random.rand(W_n, W_m)\n",
    "        concatenated_attention = np.concatenate(attentions, axis=1) @ W\n",
    "\n",
    "        return concatenated_attention\n",
    "    \n",
    "\n",
    "    def decoder_layer(decoder_inputs, encoder_outputs, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "        Z = multi_head_attention(decoder_inputs, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "        W1 = np.random.randn(4, 8)\n",
    "        W2 = np.random.randn(8, 4)\n",
    "        b1 = np.random.randn(8)\n",
    "        b2 = np.random.randn(4)\n",
    "\n",
    "        output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "        output = layer_norm(output, Z)\n",
    "\n",
    "        Z_encoder_decoder = multi_head_encoder_decoder_attention(encoder_outputs, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "\n",
    "        Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z)\n",
    "\n",
    "        output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "        return layer_norm(output + Z_encoder_decoder)\n",
    "    \n",
    "\n",
    "    def decoder(decoder_input, output_encoder, n=6):\n",
    "        for _ in range(n):\n",
    "            decoder_input = decoder_layer(decoder_input, output_encoder, 2, 4, 3, 6, 4)\n",
    "\n",
    "        return decoder_input\n",
    "\n",
    "\n",
    "    def linear(x, W, b):\n",
    "        return np.dot(x, W) + b\n",
    "\n",
    "\n",
    "    def output_probabilities(decoder_outputs, embeddings_dim, vocabulary_length):\n",
    "        for decoder_output in decoder_outputs:\n",
    "            linear_output = linear(decoder_output, np.random.randn(embeddings_dim, vocabulary_length), np.random.randn(vocabulary_length))\n",
    "\n",
    "            softmax_output = softmax(linear_output, axis=0)\n",
    "\n",
    "            return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer =  SimpleTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Привет мир'\n",
    "transformer_outputs = transformer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
