{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация архитектуры Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://habrastorage.org/getpro/habr/upload_files/f7a/3a5/e84/f7a3a5e845433fa313070f9c794a5fb7.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В модель подаются текстовые данные. Будем использовать классику - \"Привет, мир!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Привет мир'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Входные данные необходимо токенизировать и в качестве примера возьмем базовую токенизацию из библиотеки nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzier = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzie_seq = tokenzier.tokenize(inputs.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем набор токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет', 'мир']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenzie_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В оригинальной архитектуре используются эмбеддинги размером 512, но в качестве простоты буду использовать 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имитируем создание эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = np.random.rand(len(tokenzie_seq), embeddings_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89700025, 0.74198298, 0.87911062, 0.51858262],\n",
       "       [0.52839169, 0.53158087, 0.11828162, 0.82088   ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### После получения входящих эмбеддингов следует позиционное кодирование, используем классический способ с использованием синуса и косинуса\n",
    "\n",
    "<img src=\"https://habrastorage.org/getpro/habr/upload_files/077/39a/c8c/07739ac8ce5c811f974ff3d5150159cc.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(input_embeddings):\n",
    "    seq_len, embeddings_dim = input_embeddings.shape\n",
    "\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    \n",
    "    div_term = np.exp(np.arange(0, embeddings_dim, 2) * - (np.log(10000.0) / embeddings_dim))\n",
    "    \n",
    "    PE = np.zeros((seq_len, embeddings_dim))\n",
    "    PE[:, 0::2] = np.sin(position * div_term) \n",
    "    \n",
    "    PE[:, 1::2] = np.cos(position * div_term) \n",
    "    \n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings += positional_encoding(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89700025, 1.74198298, 0.87911062, 1.51858262],\n",
       "       [1.36986267, 1.07188318, 0.12828145, 1.82083   ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Готово! Теперь можем подавать данные в саму модель. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура Transformers делится на 2 основных блока - энкодер и декодер. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первым, что нас встречает в энкодере - многоголовое внимание. Начнем с 2х голов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heads(heads_numbers, heads_n, heads_m):\n",
    "    WK = []\n",
    "    WQ = []\n",
    "    WV = []\n",
    "\n",
    "    for _ in range(heads_numbers):\n",
    "        WK_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "        WQ_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "        WV_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "\n",
    "        WK.append(WK_temp)\n",
    "        WQ.append(WQ_temp)\n",
    "        WV.append(WV_temp)\n",
    "\n",
    "    return np.array(WK), np.array(WQ), np.array(WV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK, WQ, WV = create_heads(2, 4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем матрицы WK, WQ, WV для каждой из головы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1, WK2 = WK[0], WK[1]\n",
    "WQ1, WQ2 = WQ[0], WQ[1]\n",
    "WV1, WV2 = WV[0], WV[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь необходимо умножить входящую последовательность на каждую из матриц для получения ключей, значений и запросов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ключ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.77611087, 1.74198298, 4.13967622],\n",
       "       [1.49814412, 1.07188318, 3.02099463]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1 = input_embeddings @ WK1\n",
    "K1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.39769324, 2.39769324, 1.77611087],\n",
       "       [1.94911145, 1.94911145, 1.49814412]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = input_embeddings @ WV1\n",
    "V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запрос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.41558287, 0.        , 3.2605656 ],\n",
       "       [3.19069267, 0.        , 2.89271318]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = input_embeddings @ WQ1\n",
    "Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скалярное произведение запроса и каждого ключа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.78802885, 13.46904244],\n",
       "       [17.64191988, 13.51898845]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = Q1 @ K1.T\n",
    "scores1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деление на квадратный корень размерности вектора ключа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.89401442, 6.73452122],\n",
       "       [8.82095994, 6.75949423]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = scores1 / np.sqrt(embeddings_dim)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Далее используем softmax для нормализации, чтобы все они были положительны и в сумме равнялись 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89655255, 0.10344745],\n",
       "       [0.88710105, 0.11289895]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = softmax(scores1)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление внимания!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.3512886 , 2.3512886 , 1.74735592],\n",
       "       [2.34704883, 2.34704883, 1.74472871]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = scores1 @ V1\n",
    "attention1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь обернем все в одну функцию для одной головы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(input_embeddings, WK, WQ, WV):\n",
    "    K = input_embeddings @ WK\n",
    "    Q = input_embeddings @ WQ\n",
    "    V = input_embeddings @ WV\n",
    "    \n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(embeddings_dim)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И получаем тоже самое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.3512886 , 2.3512886 , 1.74735592],\n",
       "       [2.34704883, 2.34704883, 1.74472871]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = attention(input_embeddings, WK1, WQ1, WV1)\n",
    "attention1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычислим внимание второй головы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.42414537, 1.10642277, 1.27340492],\n",
       "       [4.46548777, 1.1493195 , 1.36436631]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention2 = attention(input_embeddings, WK2, WQ2, WV2)\n",
    "attention2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В конце матрицы внимания конкатенируются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.3512886 , 2.3512886 , 1.74735592, 4.42414537, 1.10642277,\n",
       "        1.27340492],\n",
       "       [2.34704883, 2.34704883, 1.74472871, 4.46548777, 1.1493195 ,\n",
       "        1.36436631]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = np.concatenate([attention1, attention2], axis=1)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Осталось реализовать функцию многоголового внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "    attentions = []\n",
    "    for i in range(len(WK)):\n",
    "        WK_cur = WK[i]\n",
    "        WQ_cur = WQ[i]\n",
    "        WV_cur = WV[i]\n",
    "\n",
    "        scores_cur = attention(input_embeddings, WK_cur, WQ_cur, WV_cur)\n",
    "        attentions.append(scores_cur)\n",
    "\n",
    "    W = np.random.rand(W_n, W_m)\n",
    "\n",
    "    return np.concatenate(attentions, axis=1) @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = multi_head_attention(input_embeddings, 2, 4, 3, 6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.8359728 , 3.41152641, 8.93632016, 6.14191246],\n",
       "       [8.82947139, 3.45527595, 8.93877201, 6.21770766]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Далее идет слой Add & Norm, который нормализует значения. (Интересный факт! Если этого слоя бы не было, то на выходе из энкодера в матрице будут nan. Связано это с тем, что значения становятся слишком большими)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(input_embeddings, epsilon=1e-6):\n",
    "    mean = input_embeddings.mean(axis=-1, keepdims=True)\n",
    "    std = input_embeddings.std(axis=-1, keepdims=True)\n",
    "    \n",
    "    return (input_embeddings - mean) / (std + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.95438355,  1.27230594, -1.0015261 ,  0.6836037 ],\n",
       "       [ 0.43897274, -0.0416654 , -1.56368617,  1.16637883]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm(input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Осталось добавить сеть прямого распространения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[81.40846137, -2.55936503, 38.31852649,  0.66808032],\n",
       "       [82.04942637, -2.51340001, 38.87532749,  0.67055369]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = feed_forward(Z, W1, b1, W2, b2)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.52671779, -0.94096884,  0.26037018, -0.84611913],\n",
       "       [ 1.52431641, -0.94131278,  0.26547345, -0.84847708]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = layer_norm(output_encoder)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь соберем энкодер воедино!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    Z = multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "    W1 = np.random.randn(4, 8)\n",
    "    W2 = np.random.randn(8, 4)\n",
    "    b1 = np.random.randn(8)\n",
    "    b2 = np.random.randn(4)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "    return layer_norm(output, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.35783162, -1.11614541, -0.11406513, -0.09956317],\n",
       "       [ 1.35763904, -1.11612881, -0.11425058, -0.09921788]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В классической архитектуре использовалось 6 слоев энкодера, поэтому сделаем тоже самое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_embeddings, n=6):\n",
    "    for _ in range(n):\n",
    "        input_embeddings = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "\n",
    "    return input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.94001839,  0.60686808,  1.91929143, -0.37666407],\n",
       "       [-1.94001839,  0.60686808,  1.91929143, -0.37666407]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = encoder(input_embeddings)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Начало декодера - выходные эмбеддинги, но в качестве первого токена на вход этому слою подается специальный токен sos, который означает начало последовательности (start of the sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_embedding = np.random.rand(len(tokenzie_seq), embeddings_dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первая часть идентична энкодеру - многоголовое внимание и нормализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65295661, 1.96540852, 0.52657657, 1.81403241],\n",
       "       [1.33200423, 1.53020025, 0.58657478, 1.48053814]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_embedding += positional_encoding(sos_embedding)\n",
    "sos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_self_attention = multi_head_attention(sos_embedding, 2, 4, 3, 6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.64862277, 5.90350643, 6.78472443, 5.96427115],\n",
       "       [7.63867652, 5.91584367, 6.78332633, 5.97368071]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3834969 ,  0.1532345 , -1.4323164 , -0.104415  ],\n",
       "       [ 1.72996616, -0.54288109, -0.65639073, -0.53069434]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_self_attention = layer_norm(decoder_self_attention + sos_embedding)\n",
    "decoder_self_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Самое интересное начинается здесь! Это внимание энкодера-декодера, куда поступают выходы из энкодера. Ключевое отличие от собственного внимания - ключ и значение получаются из выхода энкодера, а запрос формируется из поступающих данных от декодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n",
    "    K = encoder_output @ WK    \n",
    "    V = encoder_output @ WV    \n",
    "    Q = attention_input @ WQ   \n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(embeddings_dim)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_encoder_decoder_attention(encoder_output, attention_input, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "    attentions = []\n",
    "    for i in range(len(WK)):\n",
    "        WK_cur = WK[i]\n",
    "        WQ_cur = WQ[i]\n",
    "        WV_cur = WV[i]\n",
    "        \n",
    "        scores_cur = encoder_decoder_attention(encoder_output, attention_input, WK_cur, WQ_cur, WV_cur)\n",
    "        attentions.append(scores_cur)\n",
    "    \n",
    "    W = np.random.rand(W_n, W_m)\n",
    "    concatenated_attention = np.concatenate(attentions, axis=1) @ W\n",
    "\n",
    "    return concatenated_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.07226084, 1.68657209, 1.5382457 , 2.50516641],\n",
       "       [3.07226084, 1.68657209, 1.5382457 , 2.50516641]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_encoder_decoder = multi_head_encoder_decoder_attention(output_encoder, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь сформируем слой декодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(decoder_inputs, encoder_outputs, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    Z = multi_head_attention(decoder_inputs, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "    W1 = np.random.randn(4, 8)\n",
    "    W2 = np.random.randn(8, 4)\n",
    "    b1 = np.random.randn(8)\n",
    "    b2 = np.random.randn(4)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "    output = layer_norm(output, Z)\n",
    "\n",
    "    Z_encoder_decoder = multi_head_encoder_decoder_attention(encoder_outputs, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "\n",
    "    Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z)\n",
    "\n",
    "    output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z_encoder_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48526621, -1.22226021,  1.49124699,  0.21627943],\n",
       "       [-0.48534158, -1.22136576,  1.49221279,  0.21449455]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_decoder = decoder_layer(sos_embedding, output_encoder, 2, 4, 3, 6, 4)\n",
    "output_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аналогично с энкодером - декодер имеет 6 частей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(decoder_input, output_encoder, n=6):\n",
    "    for _ in range(n):\n",
    "        decoder_input = decoder_layer(decoder_input, output_encoder, 2, 4, 3, 6, 4)\n",
    "\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.14222914,  1.47216652, -0.65159337,  0.32165598],\n",
       "       [-1.14222914,  1.47216654, -0.65159334,  0.32165593]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs = decoder(sos_embedding, output_decoder)\n",
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### На выходе нам необходимо добавить линейный слой, чтобы преобразовать выходные данные в вектор размера словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, W, b):\n",
    "    return np.dot(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### А затем получаем вероятности токенов с помощью функции активации softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probabilities(decoder_outputs, embeddings_dim, vocabulary_length):\n",
    "    for decoder_output in decoder_outputs:\n",
    "        linear_output = linear(decoder_output, np.random.randn(embeddings_dim, vocabulary_length), np.random.randn(vocabulary_length))\n",
    "\n",
    "        softmax_output = softmax(linear_output, axis=0)\n",
    "\n",
    "        print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13389328 0.37876943 0.33216612 0.15517117]\n",
      "[0.42477685 0.00193619 0.18435459 0.38893238]\n"
     ]
    }
   ],
   "source": [
    "output_probabilities(decoder_outputs, embeddings_dim, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, embeddings_dim):\n",
    "        self.embeddings_dim = embeddings_dim # длина эмбеддингов\n",
    "\n",
    "        self.words_list_ru = ['<sos>', 'привет', 'мир', '<eos>', '<unk>'] # словарь первого языка \n",
    "        self.words_list_en = ['<sos>', 'hello', 'world', '<eos>', '<unk>'] # словарь второго языка\n",
    "\n",
    "        self.create_input_ids_dict() # создаем словарь id токенов\n",
    "        self.create_embeddings() # создаем \"матрицу\" эмбеддингов\n",
    "\n",
    "\n",
    "    def get_vocab_length(self):\n",
    "        return len(self.words_list_ru) # возвращаем длину словаря (необходимо для softmax по выходу из декодера)\n",
    "\n",
    "\n",
    "    def create_input_ids_dict(self):\n",
    "        self.input_ids_dict = {}\n",
    "        for i in range(len(self.words_list_ru)):\n",
    "            self.input_ids_dict[i] = self.words_list_ru[i] # формируем словарь id токенов\n",
    "\n",
    "\n",
    "    def get_input_id(self, token):\n",
    "        return list(self.input_ids_dict.keys())[list(self.input_ids_dict.values()).index(token)] # возвращаем id по токену\n",
    "\n",
    "\n",
    "    def get_input_ids(self, tokens):\n",
    "        input_ids = []\n",
    "        for token in tokens:\n",
    "            input_ids.append(self.get_input_id(token))\n",
    "        return input_ids # возвращаем все id токенов входной последовательности\n",
    "\n",
    "\n",
    "    def create_embeddings(self):\n",
    "        self.embeddings_dict = {}\n",
    "        for i in range(len(self.words_list_ru)):\n",
    "            self.embeddings_dict[i] = np.random.rand(self.embeddings_dim) # создаем \"матрицу\" эмбеддингов\n",
    "\n",
    "\n",
    "    def get_embeddings(self, input_ids):\n",
    "        embeddings = []\n",
    "        for input_id in input_ids:\n",
    "            embeddings.append(self.embeddings_dict[input_id])\n",
    "        return np.array(embeddings) # возвращаем эмбеддинги\n",
    "\n",
    "\n",
    "    def __call__(self, tokens):\n",
    "        input_ids = self.get_input_ids(tokens)\n",
    "        return self.get_embeddings(input_ids)\n",
    "    \n",
    "\n",
    "    def get_translation(self, input_id):\n",
    "        return self.words_list_en[input_id] # получаем перевод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer():\n",
    "    def __init__(self, \n",
    "                 embeddings_dim=4,\n",
    "                 heads_numbers=2,\n",
    "                 heads_n=4,\n",
    "                 heads_m=3,\n",
    "                 W_n=6,\n",
    "                 W_m=4):\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        self.embeddings_dim = embeddings_dim\n",
    "        self.embeddings = Embeddings(self.embeddings_dim)\n",
    "\n",
    "        self.heads_numbers = heads_numbers\n",
    "        self.head_n = heads_n\n",
    "        self.head_m = heads_m\n",
    "        self.W_n = W_n\n",
    "        self.W_m = W_m\n",
    "        self.vocab_length = self.embeddings.get_vocab_length()\n",
    "        \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        self.tokenized_seq = self.tokenizer.tokenize(inputs.lower())\n",
    "        self.input_embeddings = self.embeddings(self.tokenized_seq)\n",
    "\n",
    "        # пропускаем через энкодер\n",
    "        encoder_output = self.encoder(self.input_embeddings)\n",
    "        \n",
    "        # пропускаем через декодер\n",
    "        decoder_output = self.decoder(encoder_output)\n",
    "        \n",
    "        # получаем вероятности предсказания\n",
    "        probabilities = self.output_probabilities(decoder_output)\n",
    "\n",
    "        # получаем наиболее вероятные токены и вовзращаем их\n",
    "        translation = self.translate(probabilities)\n",
    "        \n",
    "        return translation\n",
    "\n",
    "\n",
    "    def translate(self, probabilities):\n",
    "        predicted_ids = np.argmax(probabilities, axis=1) # получаем id наиболее вероятных токены последовательности\n",
    "\n",
    "        translated_tokens = [self.embeddings.get_translation(idx) for idx in predicted_ids] # ищем их в словаре\n",
    "\n",
    "        return \" \".join(translated_tokens)\n",
    "\n",
    "    # далее идут функции, что использовать ранее, поэтому нет особого смысла их комментировать\n",
    "\n",
    "    def positional_encoding(self, input_embeddings):\n",
    "        seq_len, embeddings_dim = input_embeddings.shape\n",
    "\n",
    "        position = np.arange(seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, embeddings_dim, 2) * -(np.log(10000.0) / embeddings_dim))\n",
    "\n",
    "        PE = np.zeros((seq_len, embeddings_dim))\n",
    "        PE[:, 0::2] = np.sin(position * div_term) \n",
    "        PE[:, 1::2] = np.cos(position * div_term) \n",
    "        \n",
    "        return PE\n",
    "    \n",
    "    def create_heads(self):\n",
    "        WK = []\n",
    "        WQ = []\n",
    "        WV = []\n",
    "\n",
    "        for _ in range(self.heads_numbers):\n",
    "            WK_temp = np.random.randn(self.head_n, self.head_m)\n",
    "            WQ_temp = np.random.randn(self.head_n, self.head_m)\n",
    "            WV_temp = np.random.randn(self.head_n, self.head_m)\n",
    "\n",
    "            WK.append(WK_temp)\n",
    "            WQ.append(WQ_temp)\n",
    "            WV.append(WV_temp)\n",
    "\n",
    "        return np.array(WK), np.array(WQ), np.array(WV)\n",
    "    \n",
    "\n",
    "    def softmax(self, x, axis=1):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "    def attention(self, input_embeddings, WK, WQ, WV):\n",
    "        K = input_embeddings @ WK\n",
    "        Q = input_embeddings @ WQ\n",
    "        V = input_embeddings @ WV\n",
    "        \n",
    "        scores = Q @ K.T\n",
    "        scores = scores / np.sqrt(self.embeddings_dim)\n",
    "        scores = self.softmax(scores)\n",
    "        scores = scores @ V\n",
    "        return scores\n",
    "    \n",
    "\n",
    "    def multi_head_attention(self, input_embeddings):\n",
    "        WK, WQ, WV = self.create_heads()\n",
    "\n",
    "        attentions = []\n",
    "        for i in range(len(WK)):\n",
    "            WK_cur = WK[i]\n",
    "            WQ_cur = WQ[i]\n",
    "            WV_cur = WV[i]\n",
    "\n",
    "            scores_cur = self.attention(input_embeddings, WK_cur, WQ_cur, WV_cur)\n",
    "            attentions.append(scores_cur)\n",
    "\n",
    "        W = np.random.rand(self.W_n, self.W_m)\n",
    "\n",
    "        return np.concatenate(attentions, axis=1) @ W\n",
    "    \n",
    "\n",
    "    def layer_norm(self, input_embeddings, epsilon=1e-6):\n",
    "        mean = input_embeddings.mean(axis=-1, keepdims=True)\n",
    "        std = input_embeddings.std(axis=-1, keepdims=True)\n",
    "        \n",
    "        return (input_embeddings - mean) / (std + epsilon)\n",
    "\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "\n",
    "    def feed_forward(self, Z, W1, b1, W2, b2):\n",
    "        return self.relu(Z @ W1 + b1) @ W2 + b2\n",
    "\n",
    "\n",
    "    def encoder_layer(self, input_embeddings):\n",
    "        Z = self.multi_head_attention(input_embeddings)\n",
    "\n",
    "        W1 = np.random.randn(self.embeddings_dim, 8)\n",
    "        W2 = np.random.randn(8, self.embeddings_dim)\n",
    "        b1 = np.random.randn(8)\n",
    "        b2 = np.random.randn(self.embeddings_dim)\n",
    "\n",
    "        output = self.feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "        return self.layer_norm(output + Z)\n",
    "    \n",
    "\n",
    "    def encoder(self, input_embeddings, n=6):\n",
    "        for _ in range(n):\n",
    "            input_embeddings = self.encoder_layer(input_embeddings)\n",
    "\n",
    "        return input_embeddings\n",
    "\n",
    "\n",
    "    def encoder_decoder_attention(self, encoder_output, decoder_input, WK, WQ, WV):\n",
    "        K = encoder_output @ WK    \n",
    "        V = encoder_output @ WV    \n",
    "        Q = decoder_input @ WQ   \n",
    "\n",
    "        scores = Q @ K.T\n",
    "        scores = scores / np.sqrt(self.embeddings_dim)\n",
    "        scores = self.softmax(scores)\n",
    "        scores = scores @ V\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def multi_head_encoder_decoder_attention(self, encoder_output, decoder_input):\n",
    "        WK, WQ, WV = self.create_heads()\n",
    "\n",
    "        attentions = []\n",
    "        for i in range(len(WK)):\n",
    "            WK_cur = WK[i]\n",
    "            WQ_cur = WQ[i]\n",
    "            WV_cur = WV[i]\n",
    "            \n",
    "            scores_cur = self.encoder_decoder_attention(encoder_output, decoder_input, WK_cur, WQ_cur, WV_cur)\n",
    "            attentions.append(scores_cur)\n",
    "        \n",
    "        W = np.random.rand(self.W_n, self.W_m)\n",
    "        concatenated_attention = np.concatenate(attentions, axis=1) @ W\n",
    "\n",
    "        return concatenated_attention\n",
    "\n",
    "\n",
    "    def decoder_layer(self, decoder_inputs, encoder_outputs):\n",
    "        Z = self.multi_head_attention(decoder_inputs)\n",
    "\n",
    "        Z_encoder_decoder = self.multi_head_encoder_decoder_attention(encoder_outputs, Z)\n",
    "\n",
    "        W1 = np.random.randn(self.embeddings_dim, 8)\n",
    "        W2 = np.random.randn(8, self.embeddings_dim)\n",
    "        b1 = np.random.randn(8)\n",
    "        b2 = np.random.randn(self.embeddings_dim)\n",
    "\n",
    "        output = self.feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "\n",
    "        return self.layer_norm(output + Z_encoder_decoder)\n",
    "\n",
    "\n",
    "    def decoder(self, encoder_output, n=6):\n",
    "        decoder_output = np.random.rand(len(self.tokenized_seq), self.embeddings_dim)  \n",
    "        \n",
    "        for _ in range(n):\n",
    "            decoder_output = self.decoder_layer(decoder_output, encoder_output)\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "\n",
    "    def linear(self, x, W, b):\n",
    "        return np.dot(x, W) + b\n",
    "\n",
    "\n",
    "    def output_probabilities(self, decoder_outputs):\n",
    "        probabilities = []\n",
    "        for decoder_output in decoder_outputs:\n",
    "            linear_output = self.linear(decoder_output, np.random.randn(self.embeddings_dim, self.vocab_length), np.random.randn(self.vocab_length))\n",
    "            softmax_output = self.softmax(linear_output, axis=0)\n",
    "            probabilities.append(softmax_output)\n",
    "\n",
    "        return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем класс переводчика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer =  SimpleTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверяем работу!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Привет мир'\n",
    "transformer_outputs = transformer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> hello'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Благодаря собственной удаче (т.к. все мы рандомизировали и не обучали) получаем начало последовательности и hello!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
