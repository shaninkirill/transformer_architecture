{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация архитектуры Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://habrastorage.org/getpro/habr/upload_files/f7a/3a5/e84/f7a3a5e845433fa313070f9c794a5fb7.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Привет мир'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzier = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzie_seq = tokenzier.tokenize(inputs.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет', 'мир']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenzie_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = np.random.rand(len(tokenzie_seq), embeddings_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87994305, 0.9978316 , 0.83663264, 0.70755789],\n",
       "       [0.50850622, 0.27852361, 0.543555  , 0.36752233]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(input_embeddings):\n",
    "    seq_len, embeddings_dim = input_embeddings.shape\n",
    "\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    \n",
    "    div_term = np.exp(np.arange(0, embeddings_dim, 2) * - (np.log(10000.0) / embeddings_dim))\n",
    "    \n",
    "    PE = np.zeros((seq_len, embeddings_dim))\n",
    "    PE[:, 0::2] = np.sin(position * div_term) \n",
    "    \n",
    "    PE[:, 1::2] = np.cos(position * div_term) \n",
    "    \n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings += positional_encoding(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87994305, 1.9978316 , 0.83663264, 1.70755789],\n",
       "       [1.3499772 , 0.81882592, 0.55355483, 1.36747233]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heads(heads_numbers, heads_n, heads_m):\n",
    "    WK = []\n",
    "    WQ = []\n",
    "    WV = []\n",
    "\n",
    "    for _ in range(heads_numbers):\n",
    "        WK_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "        WQ_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "        WV_temp = np.random.randint(0, 2, size=(heads_n, heads_m))\n",
    "\n",
    "        WK.append(WK_temp)\n",
    "        WQ.append(WQ_temp)\n",
    "        WV.append(WV_temp)\n",
    "\n",
    "    return np.array(WK), np.array(WQ), np.array(WV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK, WQ, WV = create_heads(2, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1, WK2 = WK[0], WK[1]\n",
    "WQ1, WQ2 = WQ[0], WQ[1]\n",
    "WV1, WV2 = WV[0], WV[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7144073 , 2.87777465, 1.7165757 ],\n",
       "       [2.72235795, 2.16880312, 1.90353203]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1 = input_embeddings @ WK1\n",
    "K1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7144073 , 3.7144073 , 0.83663264],\n",
       "       [2.72235795, 2.72235795, 0.55355483]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = input_embeddings @ WV1\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.42196519, 0.83663264, 5.42196519],\n",
       "       [4.08983028, 0.55355483, 4.08983028]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = input_embeddings @ WQ1\n",
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31.85424092, 26.89590593],\n",
       "       [23.80480474, 20.11965636]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = Q1 @ K1.T\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.92712046, 13.44795297],\n",
       "       [11.90240237, 10.05982818]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = scores1 / np.sqrt(embeddings_dim)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92266842, 0.07733158],\n",
       "       [0.86325287, 0.13674713]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = softmax(scores1)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.63769055, 3.63769055, 0.81474179],\n",
       "       [3.57874739, 3.57874739, 0.79792257]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = scores1 @ V1\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(input_embeddings, WK, WQ, WV):\n",
    "    K = input_embeddings @ WK\n",
    "    Q = input_embeddings @ WQ\n",
    "    V = input_embeddings @ WV\n",
    "    \n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(embeddings_dim)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.63769055, 3.63769055, 0.81474179],\n",
       "       [3.57874739, 3.57874739, 0.79792257]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = attention(input_embeddings, WK1, WQ1, WV1)\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.67517789, 1.67517789, 3.40955395],\n",
       "       [1.66504857, 1.66504857, 3.40499305]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention2 = attention(input_embeddings, WK2, WQ2, WV2)\n",
    "attention2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.63769055, 3.63769055, 0.81474179, 1.67517789, 1.67517789,\n",
       "        3.40955395],\n",
       "       [3.57874739, 3.57874739, 0.79792257, 1.66504857, 1.66504857,\n",
       "        3.40499305]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = np.concatenate([attention1, attention2], axis=1)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "    attentions = []\n",
    "    for i in range(len(WK)):\n",
    "        WK_cur = WK[i]\n",
    "        WQ_cur = WQ[i]\n",
    "        WV_cur = WV[i]\n",
    "\n",
    "        scores_cur = attention(input_embeddings, WK_cur, WQ_cur, WV_cur)\n",
    "        attentions.append(scores_cur)\n",
    "\n",
    "    W = np.random.rand(W_n, W_m)\n",
    "\n",
    "    return np.concatenate(attentions, axis=1) @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = multi_head_attention(input_embeddings, 2, 4, 3, 6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.93599419, 8.33864912, 5.65093428, 6.95122116],\n",
       "       [5.14418379, 8.69291912, 5.87126498, 7.28813217]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(input_embeddings, epsilon=1e-6):\n",
    "    mean = input_embeddings.mean(axis=-1, keepdims=True)\n",
    "    std = input_embeddings.std(axis=-1, keepdims=True)\n",
    "    \n",
    "    return (input_embeddings - mean) / (std + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.93627259,  1.26465743, -1.02154333,  0.69315849],\n",
       "       [ 0.93802943, -0.58320927, -1.34295634,  0.98813617]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-29.94713762,  27.24728463,  19.07260353,  -0.73645552],\n",
       "       [-31.24756876,  28.46270936,  19.77488375,  -0.82670361]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = feed_forward(Z, W1, b1, W2, b2)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    Z = multi_head_attention(input_embeddings, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "    W1 = np.random.randn(4, 8)\n",
    "    W2 = np.random.randn(8, 4)\n",
    "    b1 = np.random.randn(8)\n",
    "    b2 = np.random.randn(4)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "    return layer_norm(output, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47896799,  0.75517305, -0.21408313,  0.17635716],\n",
       "       [-0.47908049,  0.75531472, -0.21419552,  0.1763379 ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_embeddings, n=6):\n",
    "    for _ in range(n):\n",
    "        input_embeddings = encoder_layer(input_embeddings, 2, 4, 3, 6, 4)\n",
    "\n",
    "    return input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.18506822,  0.28012052,  0.8675525 ,  0.78650907],\n",
       "       [-2.18506822,  0.28012052,  0.8675525 ,  0.78650907]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = encoder(input_embeddings)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_embedding = np.random.rand(len(tokenzie_seq), embeddings_dim) # start of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78417076, 1.73329351, 0.6660883 , 1.89680345],\n",
       "       [1.05468206, 1.10520399, 0.05084069, 1.34022672]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_embedding += positional_encoding(sos_embedding)\n",
    "sos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_self_attention = multi_head_attention(sos_embedding, 2, 4, 3, 6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.87277952, 6.76427698, 6.2752269 , 2.35450556],\n",
       "       [6.81397569, 6.70513658, 6.20267743, 2.3297015 ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51546707,  1.0437907 ,  0.06569568, -1.62495345],\n",
       "       [ 0.86129564,  0.82708124, -0.08629876, -1.60207811]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_self_attention = layer_norm(decoder_self_attention + sos_embedding)\n",
    "decoder_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n",
    "    K = encoder_output @ WK    \n",
    "    V = encoder_output @ WV    \n",
    "    Q = attention_input @ WQ   \n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(embeddings_dim)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_encoder_decoder_attention(encoder_output, attention_input, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    WK, WQ, WV = create_heads(heads_numbers, heads_n, heads_m)\n",
    "\n",
    "    attentions = []\n",
    "    for i in range(len(WK)):\n",
    "        WK_cur = WK[i]\n",
    "        WQ_cur = WQ[i]\n",
    "        WV_cur = WV[i]\n",
    "        \n",
    "        scores_cur = encoder_decoder_attention(encoder_output, attention_input, WK_cur, WQ_cur, WV_cur)\n",
    "        attentions.append(scores_cur)\n",
    "    \n",
    "    W = np.random.rand(W_n, W_m)\n",
    "    concatenated_attention = np.concatenate(attentions, axis=1) @ W\n",
    "\n",
    "    return concatenated_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.81472805, -1.19557598, -1.82204892, -3.21023721],\n",
       "       [-2.81472805, -1.19557598, -1.82204892, -3.21023721]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_encoder_decoder = multi_head_encoder_decoder_attention(output_encoder, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(decoder_inputs, encoder_outputs, heads_numbers, heads_n, heads_m, W_n, W_m):\n",
    "    Z = multi_head_attention(decoder_inputs, heads_numbers, heads_n, heads_m, W_n, W_m)\n",
    "\n",
    "    W1 = np.random.randn(4, 8)\n",
    "    W2 = np.random.randn(8, 4)\n",
    "    b1 = np.random.randn(8)\n",
    "    b2 = np.random.randn(4)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "    output = layer_norm(output, Z)\n",
    "\n",
    "    Z_encoder_decoder = multi_head_encoder_decoder_attention(encoder_outputs, decoder_self_attention, 2, 4, 3, 6, 4)\n",
    "\n",
    "    Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z)\n",
    "\n",
    "    output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z_encoder_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92070493,  0.97086416, -0.48714368, -1.40442541],\n",
       "       [ 0.91937023,  0.97299711, -0.48929067, -1.40307667]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_decoder = decoder_layer(sos_embedding, output_encoder, 2, 4, 3, 6, 4)\n",
    "output_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(decoder_input, output_encoder, n=6):\n",
    "    for _ in range(n):\n",
    "        decoder_input = decoder_layer(decoder_input, output_encoder, 2, 4, 3, 6, 4)\n",
    "\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.88518764, -1.6978839 ,  0.44751414,  0.36518212],\n",
       "       [ 0.88518764, -1.6978839 ,  0.44751417,  0.36518209]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs = decoder(sos_embedding, output_decoder)\n",
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, W, b):\n",
    "    return np.dot(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probabilities(decoder_outputs, embeddings_dim, vocabulary_length):\n",
    "    for decoder_output in decoder_outputs:\n",
    "        linear_output = linear(decoder_output, np.random.randn(embeddings_dim, vocabulary_length), np.random.randn(vocabulary_length))\n",
    "\n",
    "        softmax_output = softmax(linear_output, axis=0)\n",
    "\n",
    "        print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07112802 0.0397711  0.85210775 0.03699312]\n",
      "[0.03193612 0.1934926  0.04559116 0.72898012]\n"
     ]
    }
   ],
   "source": [
    "output_probabilities(decoder_outputs, embeddings_dim, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, embeddings_dim):\n",
    "        self.embeddings_dim = embeddings_dim\n",
    "\n",
    "        self.words_list_ru = ['<sos>', 'привет', 'мир', '<eos>', '<unk>']\n",
    "        self.words_list_en = ['<sos>', 'hello', 'world', '<eos>', '<unk>']\n",
    "\n",
    "        self.create_input_ids_dict()\n",
    "        self.create_embeddings()\n",
    "\n",
    "    def get_vocab_length(self):\n",
    "        return len(self.words_list_ru) \n",
    "\n",
    "    def create_input_ids_dict(self):\n",
    "        self.input_ids_dict = {}\n",
    "        for i in range(len(self.words_list_ru)):\n",
    "            self.input_ids_dict[i] = self.words_list_ru[i]\n",
    "\n",
    "    def get_input_id(self, token):\n",
    "        return list(self.input_ids_dict.keys())[list(self.input_ids_dict.values()).index(token)]\n",
    "\n",
    "    def get_input_ids(self, tokens):\n",
    "        input_ids = []\n",
    "        for token in tokens:\n",
    "            input_ids.append(self.get_input_id(token))\n",
    "        return input_ids\n",
    "\n",
    "    def create_embeddings(self):\n",
    "        self.embeddings_dict = {}\n",
    "        for i in range(len(self.words_list_ru)):\n",
    "            self.embeddings_dict[i] = np.random.rand(self.embeddings_dim)\n",
    "\n",
    "    def get_embeddings(self, input_ids):\n",
    "        embeddings = []\n",
    "        for input_id in input_ids:\n",
    "            embeddings.append(self.embeddings_dict[input_id])\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def __call__(self, tokens):\n",
    "        input_ids = self.get_input_ids(tokens)\n",
    "        return self.get_embeddings(input_ids)\n",
    "    \n",
    "    def get_translation(self, input_id):\n",
    "        return self.words_list_en[input_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer():\n",
    "    def __init__(self, \n",
    "                 embeddings_dim=4,\n",
    "                 heads_numbers=2,\n",
    "                 heads_n=4,\n",
    "                 heads_m=3,\n",
    "                 W_n=6,\n",
    "                 W_m=4):\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        self.embeddings_dim = embeddings_dim\n",
    "        self.embeddings = Embeddings(self.embeddings_dim)\n",
    "\n",
    "        self.heads_numbers = heads_numbers\n",
    "        self.head_n = heads_n\n",
    "        self.head_m = heads_m\n",
    "        self.W_n = W_n\n",
    "        self.W_m = W_m\n",
    "        self.vocab_length = self.embeddings.get_vocab_length()\n",
    "        \n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        self.tokenized_seq = self.tokenizer.tokenize(inputs.lower())\n",
    "        self.input_embeddings = self.embeddings(self.tokenized_seq)\n",
    "\n",
    "        # Пропускаем через энкодер\n",
    "        encoder_output = self.encoder(self.input_embeddings)\n",
    "        \n",
    "        # Используем предсказания декодера\n",
    "        decoder_output = self.decoder(encoder_output)\n",
    "        \n",
    "        # Получаем вероятности предсказания\n",
    "        probabilities = self.output_probabilities(decoder_output)\n",
    "\n",
    "        translation = self.translate(probabilities)\n",
    "        \n",
    "        return translation\n",
    "\n",
    "    def positional_encoding(self, input_embeddings):\n",
    "        seq_len, embeddings_dim = input_embeddings.shape\n",
    "\n",
    "        position = np.arange(seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, embeddings_dim, 2) * -(np.log(10000.0) / embeddings_dim))\n",
    "\n",
    "        PE = np.zeros((seq_len, embeddings_dim))\n",
    "        PE[:, 0::2] = np.sin(position * div_term) \n",
    "        PE[:, 1::2] = np.cos(position * div_term) \n",
    "        \n",
    "        return PE\n",
    "    \n",
    "    def create_heads(self):\n",
    "        WK = []\n",
    "        WQ = []\n",
    "        WV = []\n",
    "\n",
    "        for _ in range(self.heads_numbers):\n",
    "            WK_temp = np.random.randn(self.head_n, self.head_m)\n",
    "            WQ_temp = np.random.randn(self.head_n, self.head_m)\n",
    "            WV_temp = np.random.randn(self.head_n, self.head_m)\n",
    "\n",
    "            WK.append(WK_temp)\n",
    "            WQ.append(WQ_temp)\n",
    "            WV.append(WV_temp)\n",
    "\n",
    "        return np.array(WK), np.array(WQ), np.array(WV)\n",
    "    \n",
    "\n",
    "    def softmax(self, x, axis=1):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "    def attention(self, input_embeddings, WK, WQ, WV):\n",
    "        K = input_embeddings @ WK\n",
    "        Q = input_embeddings @ WQ\n",
    "        V = input_embeddings @ WV\n",
    "        \n",
    "        scores = Q @ K.T\n",
    "        scores = scores / np.sqrt(self.embeddings_dim)\n",
    "        scores = self.softmax(scores)\n",
    "        scores = scores @ V\n",
    "        return scores\n",
    "    \n",
    "\n",
    "    def multi_head_attention(self, input_embeddings):\n",
    "        WK, WQ, WV = self.create_heads()\n",
    "\n",
    "        attentions = []\n",
    "        for i in range(len(WK)):\n",
    "            WK_cur = WK[i]\n",
    "            WQ_cur = WQ[i]\n",
    "            WV_cur = WV[i]\n",
    "\n",
    "            scores_cur = self.attention(input_embeddings, WK_cur, WQ_cur, WV_cur)\n",
    "            attentions.append(scores_cur)\n",
    "\n",
    "        W = np.random.rand(self.W_n, self.W_m)\n",
    "\n",
    "        return np.concatenate(attentions, axis=1) @ W\n",
    "    \n",
    "\n",
    "    def layer_norm(self, input_embeddings, epsilon=1e-6):\n",
    "        mean = input_embeddings.mean(axis=-1, keepdims=True)\n",
    "        std = input_embeddings.std(axis=-1, keepdims=True)\n",
    "        \n",
    "        return (input_embeddings - mean) / (std + epsilon)\n",
    "\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "\n",
    "    def feed_forward(self, Z, W1, b1, W2, b2):\n",
    "        return self.relu(Z @ W1 + b1) @ W2 + b2\n",
    "\n",
    "\n",
    "    def encoder_layer(self, input_embeddings):\n",
    "        Z = self.multi_head_attention(input_embeddings)\n",
    "\n",
    "        W1 = np.random.randn(self.embeddings_dim, 8)\n",
    "        W2 = np.random.randn(8, self.embeddings_dim)\n",
    "        b1 = np.random.randn(8)\n",
    "        b2 = np.random.randn(self.embeddings_dim)\n",
    "\n",
    "        output = self.feed_forward(Z, W1, b1, W2, b2)\n",
    "\n",
    "        return self.layer_norm(output + Z)\n",
    "    \n",
    "\n",
    "    def encoder(self, input_embeddings, n=6):\n",
    "        for _ in range(n):\n",
    "            input_embeddings = self.encoder_layer(input_embeddings)\n",
    "\n",
    "        return input_embeddings\n",
    "\n",
    "\n",
    "    def encoder_decoder_attention(self, encoder_output, decoder_input, WK, WQ, WV):\n",
    "        K = encoder_output @ WK    \n",
    "        V = encoder_output @ WV    \n",
    "        Q = decoder_input @ WQ   \n",
    "\n",
    "        scores = Q @ K.T\n",
    "        scores = scores / np.sqrt(self.embeddings_dim)\n",
    "        scores = self.softmax(scores)\n",
    "        scores = scores @ V\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def multi_head_encoder_decoder_attention(self, encoder_output, decoder_input):\n",
    "        WK, WQ, WV = self.create_heads()\n",
    "\n",
    "        attentions = []\n",
    "        for i in range(len(WK)):\n",
    "            WK_cur = WK[i]\n",
    "            WQ_cur = WQ[i]\n",
    "            WV_cur = WV[i]\n",
    "            \n",
    "            scores_cur = self.encoder_decoder_attention(encoder_output, decoder_input, WK_cur, WQ_cur, WV_cur)\n",
    "            attentions.append(scores_cur)\n",
    "        \n",
    "        W = np.random.rand(self.W_n, self.W_m)\n",
    "        concatenated_attention = np.concatenate(attentions, axis=1) @ W\n",
    "\n",
    "        return concatenated_attention\n",
    "\n",
    "\n",
    "    def decoder_layer(self, decoder_inputs, encoder_outputs):\n",
    "        # Первое внимание — самовнимание декодера\n",
    "        Z = self.multi_head_attention(decoder_inputs)\n",
    "\n",
    "        # Внимание между кодером и декодером\n",
    "        Z_encoder_decoder = self.multi_head_encoder_decoder_attention(encoder_outputs, Z)\n",
    "\n",
    "        W1 = np.random.randn(self.embeddings_dim, 8)\n",
    "        W2 = np.random.randn(8, self.embeddings_dim)\n",
    "        b1 = np.random.randn(8)\n",
    "        b2 = np.random.randn(self.embeddings_dim)\n",
    "\n",
    "        output = self.feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "\n",
    "        return self.layer_norm(output + Z_encoder_decoder)\n",
    "\n",
    "\n",
    "    def decoder(self, encoder_output, n=6):\n",
    "        decoder_output = np.random.rand(len(self.tokenized_seq), self.embeddings_dim)  \n",
    "        \n",
    "        for _ in range(n):\n",
    "            decoder_output = self.decoder_layer(decoder_output, encoder_output)\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "\n",
    "    def linear(self, x, W, b):\n",
    "        return np.dot(x, W) + b\n",
    "\n",
    "\n",
    "    def output_probabilities(self, decoder_outputs):\n",
    "        probabilities = []\n",
    "        for decoder_output in decoder_outputs:\n",
    "            linear_output = self.linear(decoder_output, np.random.randn(self.embeddings_dim, self.vocab_length), np.random.randn(self.vocab_length))\n",
    "            softmax_output = self.softmax(linear_output, axis=0)\n",
    "            probabilities.append(softmax_output)\n",
    "\n",
    "        return np.array(probabilities)\n",
    "\n",
    "\n",
    "    def translate(self, probabilities):\n",
    "        predicted_ids = np.argmax(probabilities, axis=1)\n",
    "\n",
    "        translated_tokens = [self.embeddings.get_translation(idx) for idx in predicted_ids]\n",
    "\n",
    "        return \" \".join(translated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer =  SimpleTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Привет мир'\n",
    "transformer_outputs = transformer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
